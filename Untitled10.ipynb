{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1099e82a",
   "metadata": {},
   "source": [
    "1. Start a ChatBot session to understand what a Classification Decision Tree is: (a) ask the ChatBot to describe the type of problem a Classification Decision Tree addresses and provide some examples of real-world applications where this might be particularly useful, and then (b) make sure you understand the difference between how a Classification Decision Tree makes (classification) predictions versus how Multiple Linear Regression makes (regression) predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee10319",
   "metadata": {},
   "source": [
    "A Classification Decision Tree is a supervised machine learning algorithm used for classifying data into distinct categories. It works by splitting the dataset into subsets based on the values of the features (or attributes) in the dataset, using a tree-like structure.\n",
    "\n",
    "a）A Classification Decision Tree is a machine learning algorithm used for classification problems, where the goal is to predict the category of data based on input features. It works by recursively splitting the dataset into subsets until the data in each subset belongs to the same class or a stopping criterion is met. Classification decision trees are widely used in fields like medical diagnosis, customer churn prediction, spam detection, credit scoring, fraud detection, and marketing because they are intuitive, easy to interpret, and handle complex non-linear relationships with minimal data preprocessing.\n",
    "\n",
    "b）The key difference between how a Classification Decision Tree makes predictions versus how Multiple Linear Regression makes predictions lies in the type of output they generate and how they process the input data.\n",
    "\n",
    "https://chatgpt.com/share/673e3c1b-bf50-8013-b3a8-7fbb83280a62"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728fd876",
   "metadata": {},
   "source": [
    "2. Continue your ChatBot session and explore with your ChatBot what real-world application scenario(s) might be most appropriately addressed by each of the following metrics below: provide your answers and, in your own words, concisely explain your rationale for your answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277cd59b",
   "metadata": {},
   "source": [
    "Accuracy:\n",
    "Real-world application: General-purpose classification tasks in areas where the balance between true positives and true negatives matters equally.\n",
    "Example: A spam email filter.\n",
    "Rationale: Accuracy works well when both false positives and false negatives are equally problematic, as it measures the overall proportion of correct classifications.\n",
    "\n",
    "Sensitivity (Recall):\n",
    "Real-world application: Medical testing for diseases, especially where failing to identify a condition could be detrimental (e.g., cancer detection).\n",
    "Example: Breast cancer screening.\n",
    "Rationale: Sensitivity is crucial when the goal is to minimize false negatives, as missing a diagnosis (e.g., not identifying cancer when it's present) can have serious consequences.\n",
    "\n",
    "Specificity:\n",
    "Real-world application: Situations where you want to ensure that those who do not have a condition are correctly identified, reducing the chance of unnecessary treatments or tests.\n",
    "Example: Screening for a rare disease in a population where false positives would lead to unnecessary and costly treatments.\n",
    "Rationale: Specificity is important when the focus is on minimizing false positives, as falsely diagnosing someone with a condition could result in unnecessary follow-up tests or treatments.\n",
    "\n",
    "Precision:\n",
    "Real-world application: Spam detection or fraud detection, where false positives (misclassifying a legitimate email or transaction as fraudulent) are costly or undesirable.\n",
    "Example: Credit card fraud detection.\n",
    "Rationale: Precision is important when the cost of a false positive is high, such as falsely flagging a legitimate credit card transaction as fraudulent, which could cause inconvenience to users and financial institutions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11411a6e",
   "metadata": {},
   "source": [
    "3. Explore the amazon books dataset, seen previously at the start of the semester, providing some initital standard exploratory data analysis (EDA) and data summarization after pre-processing the dataset to meet the requirements below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84977c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import accuracy_score, recall_score, make_scorer\n",
    "import graphviz as gv\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/pointOfive/STA130_F23/main/Data/amazonbooks.csv\"\n",
    "ab = pd.read_csv(url, encoding=\"ISO-8859-1\")\n",
    "# create `ab_reduced_noNaN` based on the specs above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e9c478",
   "metadata": {},
   "source": [
    "4. Create an 80/20 split with 80% of the data as a training set ab_reduced_noNaN_train and 20% of the data testing set ab_reduced_noNaN_test using either df.sample(...) as done in TUT or using train_test_split(...) as done in the previous HW, and report on how many observations there are in the training data set and the test data set.Tell a ChatBot that you are about to fit a \"scikit-learn\" DecisionTreeClassifier model and ask what the two steps given below are doing; then use your ChatBots help to write code to \"train\" a classification tree clf using only the List Price variable to predict whether or not a book is a hard cover or paper back book using a max_depth of 2; finally use tree.plot_tree(clf) to explain what predictions are made based on List Price for the fitted clf model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81a6e468",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m ab_reduced_noNaN \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtree\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DecisionTreeClassifier\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1711\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1712\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1714\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "ab_reduced_noNaN = pd.read_csv('data.csv')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt\n",
    "ab_reduced_noNaN_train, ab_reduced_noNaN_test = train_test_split(ab_reduced_noNaN, test_size=0.2, random_state=42)\n",
    "print(f'Training set size: {len(ab_reduced_noNaN_train)}')\n",
    "print(f'Test set size: {len(ab_reduced_noNaN_test)}')\n",
    "y = pd.get_dummies(ab_reduced_noNaN[\"Hard_or_Paper\"])['H']\n",
    "X = ab_reduced_noNaN[['List Price']]\n",
    "clf = DecisionTreeClassifier(max_depth=2, random_state=42)\n",
    "clf.fit(X, y)\n",
    "plt.figure(figsize=(12,8))\n",
    "tree.plot_tree(clf, filled=True, feature_names=['List Price'], class_names=['Not H', 'H'], rounded=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222f8e14",
   "metadata": {},
   "source": [
    "y = pd.get_dummies(ab_reduced_noNaN[\"Hard_or_Paper\"])['H']: This converts the \"Hard_or_Paper\" column into a binary column where 'H' represents hardcover and 1 represents that it's hardcover.\n",
    "X = ab_reduced_noNaN[['List Price']]: This selects the List Price column as the feature for predicting whether a book is hardcover or paperback.\n",
    "DecisionTreeClassifier(max_depth=2): This initializes a decision tree model with a maximum depth of 2, meaning the tree will have at most 2 levels of splits.\n",
    "clf.fit(X, y): This fits the decision tree model to the data, using List Price to predict whether the book is hardcover or paperback.\n",
    "tree.plot_tree(clf): This plots the decision tree so you can visualize the splits based on List Price.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cbbe91",
   "metadata": {},
   "source": [
    "5. Repeat the previous problem but this time visualize the classification decision tree based on the following specifications below; then explain generally how predictions are made for the clf2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "00bae8e3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ab_reduced_noNaN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tree\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m y \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mget_dummies(\u001b[43mab_reduced_noNaN\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHard_or_Paper\u001b[39m\u001b[38;5;124m\"\u001b[39m])[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mH\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      5\u001b[0m X \u001b[38;5;241m=\u001b[39m ab_reduced_noNaN[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNumPages\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThick\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mList Price\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[1;32m      6\u001b[0m clf2 \u001b[38;5;241m=\u001b[39m DecisionTreeClassifier(max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ab_reduced_noNaN' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "import pandas as pd\n",
    "y = pd.get_dummies(ab_reduced_noNaN[\"Hard_or_Paper\"])['H']\n",
    "X = ab_reduced_noNaN[['NumPages', 'Thick', 'List Price']]\n",
    "clf2 = DecisionTreeClassifier(max_depth=4, random_state=42)\n",
    "clf2.fit(X, y)\n",
    "tree.plot_tree(clf2, feature_names=X.columns, filled=True, rounded=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf7e3c5",
   "metadata": {},
   "source": [
    "6. Use previously created ab_reduced_noNaN_test to create confusion matrices for clf and clf2. Report the sensitivity, specificity and accuracy for each of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7c5765f",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 33) (47926269.py, line 33)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[7], line 33\u001b[0;36m\u001b[0m\n\u001b[0;31m    print(f\"Accuracy: {np.round(a\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 33)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Create predictions for both classifiers\n",
    "y_true = pd.get_dummies(ab_reduced_noNaN_test[\"Hard_or_Paper\"])['H']\n",
    "y_pred_clf = clf.predict(ab_reduced_noNaN_test[['List Price']])\n",
    "y_pred_clf2 = clf2.predict(ab_reduced_noNaN_test[['NumPages', 'Thick', 'List Price']])\n",
    "\n",
    "# Generate confusion matrices for both models\n",
    "cm_clf = confusion_matrix(y_true, y_pred_clf)\n",
    "cm_clf2 = confusion_matrix(y_true, y_pred_clf2)\n",
    "\n",
    "# Extract TP, TN, FP, FN for clf\n",
    "TP_clf, FP_clf, FN_clf, TN_clf = cm_clf.ravel()\n",
    "\n",
    "# Extract TP, TN, FP, FN for clf2\n",
    "TP_clf2, FP_clf2, FN_clf2, TN_clf2 = cm_clf2.ravel()\n",
    "\n",
    "# Calculate sensitivity, specificity, and accuracy for clf\n",
    "sensitivity_clf = TP_clf / (TP_clf + FN_clf)\n",
    "specificity_clf = TN_clf / (TN_clf + FP_clf)\n",
    "accuracy_clf = (TP_clf + TN_clf) / (TP_clf + TN_clf + FP_clf + FN_clf)\n",
    "\n",
    "# Calculate sensitivity, specificity, and accuracy for clf2\n",
    "sensitivity_clf2 = TP_clf2 / (TP_clf2 + FN_clf2)\n",
    "specificity_clf2 = TN_clf2 / (TN_clf2 + FP_clf2)\n",
    "accuracy_clf2 = (TP_clf2 + TN_clf2) / (TP_clf2 + TN_clf2 + FP_clf2 + FN_clf2)\n",
    "\n",
    "# Print out the results rounded to 3 decimal places\n",
    "print(f\"Metrics for clf (Decision Tree with List Price):\")\n",
    "print(f\"Sensitivity: {np.round(sensitivity_clf, 3)}\")\n",
    "print(f\"Specificity: {np.round(specificity_clf, 3)}\")\n",
    "print(f\"Accuracy: {np.round(a\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f6dbb1",
   "metadata": {},
   "source": [
    "7. Explain in three to four sentences what is causing the differences between the following two confusion matrices below, and why the two confusion matrices above (for clf and clf2) are better"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b63e162",
   "metadata": {},
   "source": [
    "The differences between the two confusion matrices you're comparing stem from the features used to train the classifiers. In the first case, the decision tree (clf) is trained using only the List Price feature, while in the second case, the model is trained using three features: NumPages, Thick, and List Price. Including additional features (NumPages and Thick) in the second model may allow the classifier to make more nuanced predictions, potentially leading to different distributions of true positives, false positives, true negatives, and false negatives.\n",
    "\n",
    "The confusion matrices from the test set (clf and clf2) are better because they evaluate the models on unseen data, providing a more realistic measure of model performance. Training set performance (like in the confusion matrices you're comparing) can often be overly optimistic, as the model has already seen the data. The test set metrics reflect how well the models generalize to new, unseen examples, making them more reliable indicators of a model's effectiveness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1525e0",
   "metadata": {},
   "source": [
    "8. Read the paragraphs in Further Guidance and ask a ChatBot how to visualize feature Importances available for scikit-learn classification decision trees; do so for clf2; and use .feature_names_in_ corresponding to .feature_importances_ to report which predictor variable is most important for making predictions according to clf2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fc10ce",
   "metadata": {},
   "source": [
    "clf2.feature_importances_: Provides the relative importance of each feature used in the model.\n",
    "\n",
    "clf2.feature_names_in_: Contains the names of the features used to train clf2.\n",
    "\n",
    "np.argsort(importances)[::-1]: Sorts the features in descending order based on their importance.\n",
    "\n",
    "Visualization: A bar plot is used to visualize the feature importances, and the most important feature is printed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d18711",
   "metadata": {},
   "source": [
    "9. Describe the differences of interpreting coefficients in linear model regression versus feature importances in decision trees in two to three sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db89fca1",
   "metadata": {},
   "source": [
    "In linear regression, coefficients represent the direct relationship between each predictor variable and the target variable, where the magnitude and sign of each coefficient indicate the strength and direction of the effect, assuming other variables are held constant. In contrast, feature importances in decision trees reflect how much each predictor variable contributes to improving the model's predictions by being used in the decision splits, but they do not directly quantify the effect of a feature in a linear manner. While linear models offer clear, additive relationships, decision trees capture more complex, non-linear interactions between features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade4a1ff",
   "metadata": {},
   "source": [
    "10. Have you reviewed the course wiki-textbook and interacted with a ChatBot (or, if that wasn't sufficient, real people in the course piazza discussion board or TA office hours) to help you understand all the material in the tutorial and lecture that you didn't quite follow when you first saw it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84890099",
   "metadata": {},
   "source": [
    "Mostly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
